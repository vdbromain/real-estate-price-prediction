{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SAME CODE AS WEEK 2 !\n",
    "\n",
    "\n",
    "\n",
    "filename = \"../.gitignore/data_a (Florent).csv\"\n",
    "df = pd.read_csv(filename, skipinitialspace=True) #Open the csv file with no space before and after the values\n",
    "\n",
    "#Part 1 : cleaning the data\n",
    "\n",
    "#A. Drop the duplicates\n",
    "\n",
    "nb_of_duplicates = len(df)-len(df.drop_duplicates()) #How many duplicates do I have ?\n",
    "\n",
    "if nb_of_duplicates != 0 :\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "#B. Empty cells :\n",
    "\n",
    "df.isnull().sum() #How many empty cells are there for each column\n",
    "\n",
    "cleaned_df = df.dropna(subset=['Price'])#Delete the 3992 empty price\n",
    "cleaned_df = cleaned_df.dropna(subset=['Number of rooms'])#Delete the 1918 empty Number of rooms\n",
    "cleaned_df = cleaned_df.dropna(subset=['Living Area'])#Delete the 11677 empty Living Area (60605 left)\n",
    "\n",
    "cleaned_df[\"Garden\"].fillna(False, inplace = True) #Replace NaN value by False in the Garden Column\n",
    "cleaned_df[\"Furnished\"].fillna(False, inplace = True) #Replace NaN value by False in the Furnished Column\n",
    "cleaned_df[\"Fully equipped kitchen\"].fillna(False, inplace = True) #Replace NaN value by False in the Furnished Column\n",
    "\n",
    "del cleaned_df['Area of the terrace'] #Delete the column Area of the terrace\n",
    "del cleaned_df['Area of the garden'] #Delete the column Area of the garden\n",
    "#del cleaned_df['State of the building'] #Delete the column State of the building\n",
    "del cleaned_df['Surface area of the plot of land'] #Delete the column Surface area of the plot of land\n",
    "del cleaned_df['Surface of the land'] #Delete the column Surface area of land\n",
    "del cleaned_df['Number of facades'] #Delete the column Number of facades\n",
    "\n",
    "nb_null_values = cleaned_df.isnull().sum()\n",
    "#C. No blank spaces \n",
    "# Already done in the opening of the csv file\n",
    "\n",
    "#D. Data in wrong format\n",
    "\n",
    "cleaned_df_formatted = cleaned_df.convert_dtypes() #convert all the dtypes in the best types for them\n",
    "cleaned_df_formatted['Price'] = cleaned_df_formatted['Price'].astype(float)\n",
    "cleaned_df_formatted['Living Area'] = cleaned_df_formatted['Living Area'].astype(float)\n",
    "cleaned_df_formatted['Number of rooms'] = cleaned_df_formatted['Number of rooms'].astype(float)\n",
    "\n",
    "#E. Wrong data\n",
    "\n",
    "index_sell_to_del = cleaned_df_formatted[(cleaned_df_formatted['To sell'] == False)].index # taking the index of the row who are False to delete them on the next line\n",
    "df_sell = cleaned_df_formatted.drop(index_sell_to_del) #Delete the rows collect the line above and store the new DF in df_sell\n",
    "del df_sell['To rent'] #Delete the column \"To rent\" because it is useless now\n",
    "\n",
    "index_rent_to_del = cleaned_df_formatted[(cleaned_df_formatted['To rent'] == False)].index # taking the index of the row who are False to delete them on the next line\n",
    "df_rent = cleaned_df_formatted.drop(index_rent_to_del) #Delete the rows collect the line above and store the new DF in df_rent\n",
    "del df_rent['To sell'] #Delete the column \"To sell\" because it is useless now\n",
    "\n",
    "#Delete the extreme prices and extreme living area from df_rent\n",
    "\n",
    "df_rent.drop(df_rent[df_rent['Price'] >= 4000].index, inplace = True)\n",
    "df_rent.drop(df_rent[df_rent['Living Area'] > 400].index, inplace = True)\n",
    "\n",
    "#Creating a Dataframe for renting appartment and Rez-de-chaussee\n",
    "\n",
    "#Taking only the appartment and Rez-de-chaussée to rent \n",
    "df_rent_appart = df_rent[(df_rent['type'] == \"Appartement\") | (df_rent['type'] == \"Rez-de-chaussée\")].copy()\n",
    "\n",
    "#Calculating the price per squared m for the appartment and Rez-de-chaussée to rent\n",
    "df_rent_appart.loc[ : ,'Price by squared m'] = df_rent_appart.loc[ : ,'Price'] / df_rent_appart.loc[ : ,'Living Area']\n",
    "\n",
    "#Calculating the price per bedroom m for the appartment and Rez-de-chaussée to rent\n",
    "#df_rent_appart['Price by bedroom'] = df_rent_appart['Price'] / df_rent_appart['Number of rooms']\n",
    "\n",
    "\n",
    "#Creating Datafram for selling appartement and Rez-de-chaussée\n",
    "\n",
    "#Taking only the appartment and Rez-de-chausséet to sell \n",
    "df_sell_appart = df_sell[(df_sell['type'] == \"Appartement\") | (df_sell['type'] == \"Rez-de-chaussée\")]\n",
    "#df_sell_appart.drop(df_sell_appart[(df_sell_appart['Price'] < 50000)].index, inplace=True) #Deleting the rows where the price are under 50 000 because some renting appart were true to sell so...\n",
    "\n",
    "#Part 2 : Data Analysis\n",
    "\n",
    "#How many rows and columns ?\n",
    "\n",
    "def how_many_columns_rows_df(df):\n",
    "    vnames = [name for name in globals() if globals()[name] is df]#Getting the name of the df\n",
    "    print(f\"There are {df.shape[0]} rows in our database named {vnames[0]}\") #How many rows ?\n",
    "    print(f\"There are {df.shape[1]} columns in our database named {vnames[0]}\") #How many columns ?\n",
    "    return None\n",
    "\n",
    "#how_many_columns_rows_df(df_rent_appart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8676 rows in our database named df_rent_appart\n",
      "There are 15 columns in our database named df_rent_appart\n",
      "There are 15543 rows in our database named df_sell_appart\n",
      "There are 14 columns in our database named df_sell_appart\n",
      "Unnamed: 0                0.119638\n",
      "To rent                        NaN\n",
      "Price                     1.000000\n",
      "Number of rooms           0.393882\n",
      "Living Area               0.606735\n",
      "Fully equipped kitchen    0.069420\n",
      "Furnished                 0.259539\n",
      "Open fire                 0.092814\n",
      "Terrace                   0.189533\n",
      "Garden                    0.038577\n",
      "Swimming pool             0.061990\n",
      "zipcode                   0.368450\n",
      "Price by squared m        0.544780\n",
      "Name: Price, dtype: float64\n",
      "Unnamed: 0                0.119638\n",
      "Fully equipped kitchen    0.069420\n",
      "Furnished                 0.259539\n",
      "Open fire                 0.092814\n",
      "Terrace                   0.189533\n",
      "Garden                    0.038577\n",
      "Swimming pool             0.061990\n",
      "Name: Price, dtype: float64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Price by bedroom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Price by bedroom'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[985], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdel\u001b[39;00m df_rent_appart[\u001b[39m'\u001b[39m\u001b[39mTo rent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[39mdel\u001b[39;00m df_rent_appart[\u001b[39m'\u001b[39m\u001b[39mPrice by squared m\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m \u001b[39mdel\u001b[39;00m df_rent_appart[\u001b[39m'\u001b[39;49m\u001b[39mPrice by bedroom\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py:4243\u001b[0m, in \u001b[0;36mNDFrame.__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4238\u001b[0m             deleted \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   4239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m deleted:\n\u001b[1;32m   4240\u001b[0m     \u001b[39m# If the above loop ran and didn't delete anything because\u001b[39;00m\n\u001b[1;32m   4241\u001b[0m     \u001b[39m# there was no match, this call should raise the appropriate\u001b[39;00m\n\u001b[1;32m   4242\u001b[0m     \u001b[39m# exception:\u001b[39;00m\n\u001b[0;32m-> 4243\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxes[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39midelete(loc)\n\u001b[1;32m   4246\u001b[0m \u001b[39m# delete from the caches\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Price by bedroom'"
     ]
    }
   ],
   "source": [
    "#Preprocessing Data\n",
    "\n",
    "#How many rows and columns in my DF ?\n",
    "\n",
    "how_many_columns_rows_df(df_rent_appart)\n",
    "how_many_columns_rows_df(df_sell_appart)\n",
    "\n",
    "#Calculating the correlation of my dataframe(s)\n",
    "\n",
    "corr = df_rent_appart.corr(numeric_only=True).abs() #Calculate the correlation between all the variables in the DF and take the absolute value of them\n",
    "\n",
    "print(corr['Price'])\n",
    "#Seeing the results of the correlation with the price I decide to delete some columns\n",
    "#who are not meaningful in my case\n",
    "\n",
    "print(corr['Price'][corr['Price']<0.36]) #The columns that I'll delete from my df\n",
    "\n",
    "\n",
    "del df_rent_appart['Unnamed: 0']\n",
    "del df_rent_appart['Fully equipped kitchen']\n",
    "#del df_rent_appart['Open fire']\n",
    "#del df_rent_appart['Garden']\n",
    "del df_rent_appart['Swimming pool']\n",
    "del df_rent_appart['type']\n",
    "del df_rent_appart['To rent']\n",
    "del df_rent_appart['Price by squared m']\n",
    "del df_rent_appart['Price by bedroom']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/nzxkpm4s193510wrty2mgpm40000gn/T/ipykernel_17185/3492309448.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rent_appart['Province'] = Province\n"
     ]
    }
   ],
   "source": [
    "#Creating the column \"Province\"\n",
    "\n",
    "df_rent_appart[(df_rent_appart[\"zipcode\"] >= 1000) & (df_rent_appart[\"zipcode\"] <= 1299)]\n",
    "\n",
    "Province = []\n",
    "for zipcode in df_rent_appart['zipcode']:\n",
    "    if (zipcode >= 1000) & (zipcode <= 1299): Province.append('Brussels')\n",
    "    elif (zipcode >= 1300) & (zipcode <= 1499): Province.append('Brabant wallon')\n",
    "    elif (zipcode >= 2000) & (zipcode <= 2999): Province.append('Anvers')\n",
    "    elif (zipcode >= 3500) & (zipcode <= 3999): Province.append('Limbourg')\n",
    "    elif (zipcode >= 4000) & (zipcode <= 4999): Province.append('Liege')\n",
    "    elif (zipcode >= 5000) & (zipcode <= 5680): Province.append('Namur')\n",
    "    elif (zipcode >= 6600) & (zipcode <= 6999): Province.append('Luxembourg')\n",
    "    elif (zipcode >= 8000) & (zipcode <= 8999): Province.append('Flandre occiendentale')\n",
    "    elif (zipcode >= 9000) & (zipcode <= 9999): Province.append('Flandre orientale')\n",
    "    elif (zipcode >= 1500) & (zipcode <= 1999) or (zipcode >= 3000) & (zipcode <= 3499): Province.append('Brabant flamand')\n",
    "    elif (zipcode >= 6000) & (zipcode <= 6599) or (zipcode >= 7000) & (zipcode <= 7999): Province.append('Hainaut')\n",
    "\n",
    "df_rent_appart['Province'] = Province\n",
    "del df_rent_appart['zipcode'] #Not needed anymore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8676 rows in our database named df_rent_appart\n",
      "There are 19 columns in our database named df_rent_appart\n"
     ]
    }
   ],
   "source": [
    "df_rent_appart = pd.get_dummies(data = df_rent_appart, columns = ['Province'])\n",
    "df_rent_appart.head(10)\n",
    "how_many_columns_rows_df(df_rent_appart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Price', 'Number of rooms', 'Living Area', 'Furnished', 'Open fire',\n",
      "       'Terrace', 'Garden', 'State of the building', 'Province_Anvers',\n",
      "       'Province_Brabant flamand', 'Province_Brabant wallon',\n",
      "       'Province_Brussels', 'Province_Flandre occiendentale',\n",
      "       'Province_Flandre orientale', 'Province_Hainaut', 'Province_Liege',\n",
      "       'Province_Limbourg', 'Province_Luxembourg', 'Province_Namur'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '\\nBon\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[945], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression\n\u001b[1;32m     21\u001b[0m regressor \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m---> 22\u001b[0m regressor\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     24\u001b[0m \u001b[39m#Asking for the score of our model for its training part\u001b[39;00m\n\u001b[1;32m     25\u001b[0m training_score \u001b[39m=\u001b[39m regressor\u001b[39m.\u001b[39mscore(X_train, y_train)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_base.py:649\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    645\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    647\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 649\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    650\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    651\u001b[0m )\n\u001b[1;32m    653\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(\n\u001b[1;32m    654\u001b[0m     sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype, only_non_negative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    655\u001b[0m )\n\u001b[1;32m    657\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[1;32m    658\u001b[0m     X,\n\u001b[1;32m    659\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[1;32m    663\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:554\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    552\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    553\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    555\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1104\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1100\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1101\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[0;32m-> 1104\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1105\u001b[0m     X,\n\u001b[1;32m   1106\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1107\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1108\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1109\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1110\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1111\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1112\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1113\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1114\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1115\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1116\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1117\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1118\u001b[0m )\n\u001b[1;32m   1120\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1122\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    875\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    876\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    879\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    881\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '\\nBon\\n'"
     ]
    }
   ],
   "source": [
    "#Modelization part for renting appart in Belgium with LinearRegression\n",
    "\n",
    "#Defining the features and the target\n",
    "\n",
    "print(df_rent_appart.columns)\n",
    "\n",
    "X = np.array(df_rent_appart.drop(columns = 'Price')) #features = Number of rooms, Living area and zipcode\n",
    "y = np.array(df_rent_appart['Price']) #target = Renting price\n",
    "\n",
    "#print(f\"That's a sample line for the X_features : {X}\")\n",
    "\n",
    "#Splitting the data between the training part and the testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y, test_size = 0.2, random_state = 42 )\n",
    "\n",
    "\n",
    "#Calling the algorithm we want to apply on our model and applying it\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#Asking for the score of our model for its training part\n",
    "training_score = regressor.score(X_train, y_train)\n",
    "print(f\"The score of the model for training part : {training_score}\")\n",
    "\n",
    "\n",
    "#Making a prediction with our model on the testing data part\n",
    "regressor.fit(X_test, y_test)\n",
    "regressor.predict(X_test)\n",
    "\n",
    "#Asking the score of our model for its testing part\n",
    "test_score = regressor.score(X_test, y_test)\n",
    "print(f\"The score of the model for testing part : {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the model for training part : 0.617057539702543\n",
      "The score of the model for testing part : 0.6077328104664328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:634: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.122e+06, tolerance: 4.262e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#Modelization part for renting appart in Belgium with Lasso\n",
    "\n",
    "#Defining the features and the target\n",
    "\n",
    "X = np.array(df_rent_appart.drop(columns = 'Price')) #features = Number of rooms, Living area and zipcode\n",
    "y = np.array(df_rent_appart['Price']) #target = Renting price\n",
    "\n",
    "#print(f\"That's a sample line for the X_features : {X}\")\n",
    "\n",
    "#Splitting the data between the training part and the testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y, test_size = 0.2, random_state = 42 )\n",
    "\n",
    "\n",
    "#Calling the algorithm we want to apply on our model and applying it\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "regressor = Lasso(alpha=0.01)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#Asking for the score of our model for its training part\n",
    "training_score = regressor.score(X_train, y_train)\n",
    "print(f\"The score of the model for training part : {training_score}\")\n",
    "\n",
    "\n",
    "#Making a prediction with our model on the testing data part\n",
    "regressor.fit(X_test, y_test)\n",
    "regressor.predict(X_test)\n",
    "\n",
    "#Asking the score of our model for its testing part\n",
    "test_score = regressor.score(X_test, y_test)\n",
    "print(f\"The score of the model for testing part : {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the model for training part : 0.6170576830420005\n",
      "The score of the model for testing part : 0.6077329343674762\n"
     ]
    }
   ],
   "source": [
    "#Modelization part for renting appart in Belgium with Ridge Regression\n",
    "\n",
    "#Defining the features and the target\n",
    "\n",
    "X = np.array(df_rent_appart.drop(columns = 'Price')) #features = Number of rooms, Living area and zipcode\n",
    "y = np.array(df_rent_appart['Price']) #target = Renting price\n",
    "\n",
    "#print(f\"That's a sample line for the X_features : {X}\")\n",
    "\n",
    "#Splitting the data between the training part and the testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y, test_size = 0.2, random_state = 42 )\n",
    "\n",
    "\n",
    "#Calling the algorithm we want to apply on our model and applying it\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "regressor = Ridge(alpha=0.01)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#Asking for the score of our model for its training part\n",
    "training_score = regressor.score(X_train, y_train)\n",
    "print(f\"The score of the model for training part : {training_score}\")\n",
    "\n",
    "\n",
    "#Making a prediction with our model on the testing data part\n",
    "regressor.fit(X_test, y_test)\n",
    "regressor.predict(X_test)\n",
    "\n",
    "#Asking the score of our model for its testing part\n",
    "test_score = regressor.score(X_test, y_test)\n",
    "print(f\"The score of the model for testing part : {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the model for training part : 0.6166807320594537\n",
      "The score of the model for testing part : 0.6074124423052933\n"
     ]
    }
   ],
   "source": [
    "#Modelization part for renting appart in Belgium with Elastic Net\n",
    "\n",
    "#Defining the features and the target\n",
    "\n",
    "X = np.array(df_rent_appart.drop(columns = 'Price')) #features = Number of rooms, Living area and zipcode\n",
    "y = np.array(df_rent_appart['Price']) #target = Renting price\n",
    "\n",
    "#print(f\"That's a sample line for the X_features : {X}\")\n",
    "\n",
    "#Splitting the data between the training part and the testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y, test_size = 0.2, random_state = 42 )\n",
    "\n",
    "\n",
    "#Calling the algorithm we want to apply on our model and applying it\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "regressor = ElasticNet(alpha=0.01)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#Asking for the score of our model for its training part\n",
    "training_score = regressor.score(X_train, y_train)\n",
    "print(f\"The score of the model for training part : {training_score}\")\n",
    "\n",
    "\n",
    "#Making a prediction with our model on the testing data part\n",
    "regressor.fit(X_test, y_test)\n",
    "regressor.predict(X_test)\n",
    "\n",
    "#Asking the score of our model for its testing part\n",
    "test_score = regressor.score(X_test, y_test)\n",
    "print(f\"The score of the model for testing part : {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR RENT_DF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12773 rows in our database named df_rent\n",
      "There are 13 columns in our database named df_rent\n",
      "Unnamed: 0                0.074634\n",
      "To rent                        NaN\n",
      "Price                     1.000000\n",
      "Number of rooms           0.483606\n",
      "Living Area               0.608514\n",
      "Fully equipped kitchen    0.072724\n",
      "Furnished                 0.170442\n",
      "Open fire                 0.166774\n",
      "Terrace                   0.205782\n",
      "Garden                    0.152727\n",
      "Swimming pool             0.069647\n",
      "zipcode                   0.377538\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Data\n",
    "\n",
    "#How many rows and columns in my DF ?\n",
    "\n",
    "how_many_columns_rows_df(df_rent)\n",
    "\n",
    "#Calculating the correlation of my dataframe(s)\n",
    "\n",
    "corr = df_rent.corr(numeric_only=True).abs() #Calculate the correlation between all the variables in the DF and take the absolute value of them\n",
    "\n",
    "print(corr['Price'])\n",
    "#Seeing the results of the correlation with the price I decide to delete some columns\n",
    "#who are not meaningful in my case\n",
    "\n",
    "del df_rent['Unnamed: 0']\n",
    "del df_rent['Fully equipped kitchen']\n",
    "#del df_rent['Open fire']\n",
    "#del df_rent['Garden']\n",
    "del df_rent['Swimming pool']\n",
    "#del df_rent['type']\n",
    "del df_rent['To rent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the column \"Province\"\n",
    "\n",
    "Province = []\n",
    "\n",
    "for zipcode in df_rent['zipcode']:\n",
    "    if (zipcode >= 1000) & (zipcode <= 1299): Province.append('Brussels')\n",
    "    elif (zipcode >= 1300) & (zipcode <= 1499): Province.append('Brabant wallon')\n",
    "    elif (zipcode >= 2000) & (zipcode <= 2999): Province.append('Anvers')\n",
    "    elif (zipcode >= 3500) & (zipcode <= 3999): Province.append('Limbourg')\n",
    "    elif (zipcode >= 4000) & (zipcode <= 4999): Province.append('Liege')\n",
    "    elif (zipcode >= 5000) & (zipcode <= 5680): Province.append('Namur')\n",
    "    elif (zipcode >= 6600) & (zipcode <= 6999): Province.append('Luxembourg')\n",
    "    elif (zipcode >= 8000) & (zipcode <= 8999): Province.append('Flandre occiendentale')\n",
    "    elif (zipcode >= 9000) & (zipcode <= 9999): Province.append('Flandre orientale')\n",
    "    elif (zipcode >= 1500) & (zipcode <= 1999) or (zipcode >= 3000) & (zipcode <= 3499): Province.append('Brabant flamand')\n",
    "    elif (zipcode >= 6000) & (zipcode <= 6599) or (zipcode >= 7000) & (zipcode <= 7999): Province.append('Hainaut')\n",
    "\n",
    "df_rent['Province'] = Province\n",
    "del df_rent['zipcode'] #Not needed anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Price', 'Number of rooms', 'Living Area', 'Furnished', 'Open fire',\n",
       "       'Terrace', 'Garden', 'type', 'Province'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rent.isnull().sum()\n",
    "df_rent.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rent.head(2)\n",
    "\n",
    "#df_rent.groupby('type').count()\n",
    "df_rent = pd.get_dummies(data = df_rent, columns = ['Province','type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the model for training part : 0.6386669242250758\n",
      "The score of the model for testing part : 0.6123179712603337\n"
     ]
    }
   ],
   "source": [
    "#Modelization part for renting appart in Belgium with LinearRegression\n",
    "\n",
    "#Defining the features and the target\n",
    "\n",
    "X = np.array(df_rent.drop(columns = 'Price')) #features = Number of rooms, Living area and zipcode\n",
    "y = np.array(df_rent['Price']) #target = Renting price\n",
    "\n",
    "#print(f\"That's a sample line for the X_features : {X}\")\n",
    "\n",
    "#Splitting the data between the training part and the testing part\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X,y, test_size = 0.2, random_state = 42 )\n",
    "\n",
    "\n",
    "#Calling the algorithm we want to apply on our model and applying it\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "#Asking for the score of our model for its training part\n",
    "training_score = regressor.score(X_train, y_train)\n",
    "print(f\"The score of the model for training part : {training_score}\")\n",
    "\n",
    "\n",
    "#Making a prediction with our model on the testing data part\n",
    "regressor.predict(X_test)\n",
    "\n",
    "#Asking the score of our model for its testing part\n",
    "test_score = regressor.score(X_test, y_test)\n",
    "print(f\"The score of the model for testing part : {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardisation of the data\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X_train = sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)\n",
    "\n",
    "#Normalisation of the data\n",
    "#from sklearn.preprocessing import Normalizer\n",
    "#sc = Normalizer()\n",
    "#X_train = sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
